# SBO-RNN-Reformulating-Recurrent-Neural-Networks-via-Stochastic-Bilevel-Optimization

## Overview
Code for paper SBO-RNN: Reformulating Recurrent Neural Network via Stochastic Bilevel Optimization.
In this paper, we proposed a family of recurrent neural networks(RNNs), namely SBO-RNN, to improve the training stability of RNNs. With the help of Stochastic gradient descent(SGD), we managed to convert the stochasitic bilevel optimization(SBO) problem into an RNN where the feedforward and backpropagation solve the lower and upper-level optimization for learning hidden states and their hyperparameters, respectively.Empirically we demonstrate our approach with superior performance on several benchmark datasets, with fewer parameters, less training data, and much faster convergence.

![Illustration of SBO-RNN architectures using the optimizers](links)



## Installation
1. Clone repo
```
git clone --recursive 
```

## Visualize the test results

